{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marumarukun/Documents/compe/kaggle_eedi/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-10 21:35:40 _custom_ops.py:19] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 11-10 21:35:40 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:35:40,927\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_number: '001'\n",
      "run_time: base\n",
      "data:\n",
      "  input_root: ../../data/input\n",
      "  train_path: ../../data/input/train.csv\n",
      "  test_path: ../../data/input/test.csv\n",
      "  sample_submission_path: ../../data/input/sample_submission.csv\n",
      "  mapping_path: ../../data/input/misconception_mapping.csv\n",
      "  mapping_meta_path: ../../data/input/mapping_meta.parquet\n",
      "  output_root: ../../data/output\n",
      "  results_root: ../../results\n",
      "  results_path: ../../results/001/base\n",
      "seed: 42\n",
      "embed_model: BAAI/bge-large-en-v1.5\n",
      "k: 50\n",
      "llm_model: Qwen/Qwen2.5-32B-Instruct-AWQ\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import vllm\n",
    "from omegaconf import OmegaConf\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.config import cfg\n",
    "from src.data import add_subject_name_info, preprocess_train\n",
    "from src.prompt import create_prompt\n",
    "from src.seed import seed_everything\n",
    "\n",
    "cfg.exp_number = Path().resolve().name\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train_df = pl.read_csv(cfg.data.train_path, try_parse_dates=True)\n",
    "test_df = pl.read_csv(cfg.data.test_path, try_parse_dates=True)\n",
    "sample_submission_df = pl.read_csv(cfg.data.sample_submission_path, try_parse_dates=True)\n",
    "mapping_df = pl.read_csv(cfg.data.mapping_path, try_parse_dates=True)\n",
    "\n",
    "# CV\n",
    "gkf = GroupKFold(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込みモデル\n",
    "model = SentenceTransformer(cfg.embed_model, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # llmの準備\n",
    "# llm = vllm.LLM(\n",
    "#     cfg.llm_model,\n",
    "#     quantization=\"awq\",\n",
    "#     tensor_parallel_size=1,\n",
    "#     gpu_memory_utilization=0.90,\n",
    "#     trust_remote_code=True,\n",
    "#     dtype=\"half\",\n",
    "#     enforce_eager=True,\n",
    "#     max_model_len=3824,\n",
    "#     disable_log_stats=True,\n",
    "# )\n",
    "# tokenizer = llm.get_tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizerを準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.llm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, valid_idx in gkf.split(train_df, groups=train_df[\"QuestionId\"]):\n",
    "    # train_dfの分割\n",
    "    train = train_df[train_idx]\n",
    "    valid = train_df[valid_idx]\n",
    "\n",
    "    # trainのSubjectName情報をmapping_dfに追加\n",
    "    mapping_meta = add_subject_name_info(train, mapping_df)\n",
    "\n",
    "    # trainの前処理\n",
    "    train_long = preprocess_train(train)\n",
    "\n",
    "    # 埋め込みモデルでベクトル化（1st stage）\n",
    "    train_long_embed = model.encode(train_long[\"AllText\"].to_list(), normalize_embeddings=True)\n",
    "    misconception_vec = model.encode(\n",
    "        mapping_meta[\"MisconceptionName_with_SubjectNames\"].to_list(), normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # 埋め込みからTOPkを抽出\n",
    "    topk_ids = util.semantic_search(train_long_embed, misconception_vec, top_k=cfg.k)\n",
    "\n",
    "    # LLMによる絞り込み(2nd stage)\n",
    "\n",
    "    # promptを作成\n",
    "    train_long = create_prompt(topk_ids, mapping_meta, train_long, tokenizer, cfg.k)\n",
    "    break\n",
    "train_long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_long\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "train_long.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
