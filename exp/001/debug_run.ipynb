{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marumarukun/Documents/compe/kaggle_eedi/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-10 21:35:40 _custom_ops.py:19] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 11-10 21:35:40 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:35:40,927\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_number: '001'\n",
      "run_time: base\n",
      "data:\n",
      "  input_root: ../../data/input\n",
      "  train_path: ../../data/input/train.csv\n",
      "  test_path: ../../data/input/test.csv\n",
      "  sample_submission_path: ../../data/input/sample_submission.csv\n",
      "  mapping_path: ../../data/input/misconception_mapping.csv\n",
      "  mapping_meta_path: ../../data/input/mapping_meta.parquet\n",
      "  output_root: ../../data/output\n",
      "  results_root: ../../results\n",
      "  results_path: ../../results/001/base\n",
      "seed: 42\n",
      "embed_model: BAAI/bge-large-en-v1.5\n",
      "k: 50\n",
      "llm_model: Qwen/Qwen2.5-32B-Instruct-AWQ\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import vllm\n",
    "from omegaconf import OmegaConf\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.config import cfg\n",
    "from src.data import add_subject_name_info, preprocess_train\n",
    "from src.prompt import create_prompt\n",
    "from src.seed import seed_everything\n",
    "\n",
    "cfg.exp_number = Path().resolve().name\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train_df = pl.read_csv(cfg.data.train_path, try_parse_dates=True)\n",
    "test_df = pl.read_csv(cfg.data.test_path, try_parse_dates=True)\n",
    "sample_submission_df = pl.read_csv(cfg.data.sample_submission_path, try_parse_dates=True)\n",
    "mapping_df = pl.read_csv(cfg.data.mapping_path, try_parse_dates=True)\n",
    "\n",
    "# CV\n",
    "gkf = GroupKFold(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込みモデル\n",
    "model = SentenceTransformer(cfg.embed_model, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmの準備\n",
    "llm = vllm.LLM(\n",
    "    cfg.llm_model,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=3500,\n",
    "    disable_log_stats=True,\n",
    ")\n",
    "# tokenizer = llm.get_tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizerを準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.llm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(response_text):\n",
    "    try:\n",
    "        # テキストをクリーンアップ\n",
    "        cleaned_text = re.sub(r\"[^0-9,]\", \"\", response_text)\n",
    "        cleaned_text = re.sub(r\",+\", \",\", cleaned_text)\n",
    "        cleaned_text = cleaned_text.strip(\",\")\n",
    "\n",
    "        # 数値のリストに変換\n",
    "        numbers = [int(num.strip()) for num in cleaned_text.split(\",\") if num.strip()]\n",
    "\n",
    "        # バリデーション\n",
    "        if not numbers:\n",
    "            return list(range(25))  # フォールバック：最初の25個を返す\n",
    "\n",
    "        # 1以上かつk未満の数値のみを保持\n",
    "        numbers = [n for n in numbers if 0 < n < cfg.k]  # kより大きい値を除外\n",
    "\n",
    "        # 25個になるように調整\n",
    "        if len(numbers) > 25:\n",
    "            numbers = numbers[:25]\n",
    "        elif len(numbers) < 25:\n",
    "            # 不足分は直前の数値で補完\n",
    "            last_num = numbers[-1] if numbers else 1  # 空リストの場合は1を使用\n",
    "            while len(numbers) < 25:\n",
    "                numbers.append(last_num)\n",
    "\n",
    "        # インデックスを0ベースに変換\n",
    "        return [num - 1 for num in numbers]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        return list(range(25))  # エラー時は最初の25個を返す\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, valid_idx in gkf.split(train_df, groups=train_df[\"QuestionId\"]):\n",
    "    # train_dfの分割\n",
    "    train = train_df[train_idx]\n",
    "    valid = train_df[valid_idx]\n",
    "\n",
    "    # trainのSubjectName情報をmapping_dfに追加\n",
    "    mapping_meta = add_subject_name_info(train, mapping_df)\n",
    "\n",
    "    # trainの前処理\n",
    "    train_long = preprocess_train(train)\n",
    "\n",
    "    # 埋め込みモデルでベクトル化（1st stage）\n",
    "    train_long_embed = model.encode(train_long[\"AllText\"].to_list(), normalize_embeddings=True)\n",
    "    misconception_vec = model.encode(\n",
    "        mapping_meta[\"MisconceptionName_with_SubjectNames\"].to_list(), normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # 埋め込みからTOPkを抽出\n",
    "    topk_ids = util.semantic_search(train_long_embed, misconception_vec, top_k=cfg.k)\n",
    "\n",
    "    # promptを作成\n",
    "    train_long = create_prompt(topk_ids, mapping_meta, train_long, tokenizer, cfg.k)\n",
    "\n",
    "    # LLMによる絞り込み(2nd stage)\n",
    "    responses = llm.generate(\n",
    "        train_long[\"prompt\"].to_numpy(),\n",
    "        vllm.SamplingParams(\n",
    "            n=1,  # Number of output sequences to return for each prompt.\n",
    "            top_p=0.8,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "            temperature=0,  # randomness of the sampling\n",
    "            seed=cfg.seed,  # Seed for reprodicibility\n",
    "            skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "            max_tokens=512,  # Maximum number of tokens to generate per output sequence.\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "    )\n",
    "    # LLMが返した生の数値をリストに格納する\n",
    "    llm_raw_responses = [extract_numbers(response.outputs[0].text) for response in responses]\n",
    "    # 対応するmisconceptionIDに変換する\n",
    "    misconception_top25_list = []\n",
    "    for i, llm_raw_response in enumerate(llm_raw_responses):\n",
    "        misconception_top25 = [str(topk_ids[i][row_n][\"corpus_id\"]) for row_n in llm_raw_response]\n",
    "        misconception_top25_list.append(misconception_top25)\n",
    "\n",
    "    # TODO: CVの設計\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
