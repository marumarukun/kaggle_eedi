{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marumarukun/pj/compe/kaggle_eedi/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_number: '000'\n",
      "run_time: base\n",
      "data:\n",
      "  input_root: ../../data/input\n",
      "  train_path: ../../data/input/train.csv\n",
      "  test_path: ../../data/input/test.csv\n",
      "  sample_submission_path: ../../data/input/sample_submission.csv\n",
      "  mapping_path: ../../data/input/misconception_mapping.csv\n",
      "  output_root: ../../data/output\n",
      "  results_root: ../../results\n",
      "  results_path: ../../results/000/base\n",
      "seed: 42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pytz\n",
    "import seaborn as sns\n",
    "from omegaconf import OmegaConf\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from src.config import cfg\n",
    "from src.data import add_subject_name_info, preprocess_train\n",
    "from src.dir import create_dir\n",
    "from src.seed import seed_everything\n",
    "\n",
    "cfg.exp_number = Path().resolve().name\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "seed_everything(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train_df = pl.read_csv(cfg.data.train_path, try_parse_dates=True)\n",
    "test_df = pl.read_csv(cfg.data.test_path, try_parse_dates=True)\n",
    "sample_submission_df = pl.read_csv(cfg.data.sample_submission_path, try_parse_dates=True)\n",
    "mapping_df = pl.read_csv(cfg.data.mapping_path, try_parse_dates=True)\n",
    "\n",
    "# CV\n",
    "gkf = GroupKFold(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較したい埋め込みモデルをここに追加していく(MTEBランクは2024/11/09時点)\n",
    "model_names = [\n",
    "    \"BAAI/bge-large-en-v1.5\",  # MTEB rank: 42, Model size: 335(Million parameters)\n",
    "    \"dunzhang/stella_en_400M_v5\",  # MTEB rank: 6, Model size: 435(Million parameters)\n",
    "    # \"dunzhang/stella_en_1.5B_v5\",  # MTEB rank: 3, Model size: 1543(Million parameters)\n",
    "    \"Alibaba-NLP/gte-large-en-v1.5\",  # MTEB rank: 28, Model size: 434(Million parameters)\n",
    "    \"jinaai/jina-embeddings-v3\",  # MTEB rank: 25, Model size: 572(Million parameters)\n",
    "]\n",
    "task = \"text-matching\"  # jina-embeddings-v3にはtaskが必要そう\n",
    "\n",
    "# NOTE: ローカルでは動作しないので、stella_en_1.5B_v5は一旦除外\n",
    "\n",
    "\n",
    "# # modelのロードと埋め込みができるか試す\n",
    "# for model_name in model_names:\n",
    "#     model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "#     print(f\"モデル: {model_name} ロードOK\")\n",
    "#     embed_trial = model.encode(train_df[\"SubjectName\"].to_list()[:5], normalize_embeddings=True)\n",
    "#     print(f\"{model_name} 埋め込みテストOK\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデル: BAAI/bge-large-en-v1.5\n",
      "Fold 1: 0.927023945267959\n",
      "Fold 2: 0.9169530355097365\n",
      "Fold 3: 0.9174548581255374\n",
      "Fold 4: 0.9235832856325129\n",
      "Fold 5: 0.9305118673148413\n",
      "CVスコア: 0.9231053983701175\n",
      "\n",
      "モデル: dunzhang/stella_en_400M_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marumarukun/pj/compe/kaggle_eedi/.venv/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/marumarukun/pj/compe/kaggle_eedi/.venv/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 0.9586659064994298\n",
      "Fold 2: 0.9596219931271478\n",
      "Fold 3: 0.9535683576956148\n",
      "Fold 4: 0.9582140812821981\n",
      "Fold 5: 0.9616814412353446\n",
      "CVスコア: 0.9583503559679469\n",
      "\n",
      "モデル: Alibaba-NLP/gte-large-en-v1.5\n",
      "Fold 1: 0.9384264538198404\n",
      "Fold 2: 0.9338487972508591\n",
      "Fold 3: 0.9392376038979651\n",
      "Fold 4: 0.9404693760732684\n",
      "Fold 5: 0.9453817557906777\n",
      "CVスコア: 0.9394727973665221\n",
      "\n",
      "モデル: jinaai/jina-embeddings-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 0.9484036488027366\n",
      "Fold 2: 0.9478808705612829\n",
      "Fold 3: 0.9438234451132129\n",
      "Fold 4: 0.951345163136806\n",
      "Fold 5: 0.9559622533600228\n",
      "CVスコア: 0.9494830761948123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 埋め込みモデルの比較をCVで行う\n",
    "\n",
    "# QuestionIdでGroupKFold\n",
    "for model_name in model_names:\n",
    "    print(f\"モデル: {model_name}\")\n",
    "\n",
    "    model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "\n",
    "    cv_scores = []\n",
    "    for i, (train_idx, valid_idx) in enumerate(gkf.split(train_df, groups=train_df[\"QuestionId\"])):\n",
    "        # train_dfの分割\n",
    "        train = train_df[train_idx]\n",
    "        valid = train_df[valid_idx]\n",
    "\n",
    "        # trainのSubjectName情報をmapping_dfに追加\n",
    "        mapping_meta = add_subject_name_info(train, mapping_df)\n",
    "\n",
    "        # trainの前処理\n",
    "        train_long = preprocess_train(train)\n",
    "\n",
    "        # 埋め込みモデルでベクトル化\n",
    "        train_long_embed = model.encode(train_long[\"AllText\"].to_list(), normalize_embeddings=True)\n",
    "        misconception_vec = model.encode(\n",
    "            mapping_meta[\"MisconceptionName_with_SubjectNames\"].to_list(), normalize_embeddings=True\n",
    "        )\n",
    "        # jina-embeddings-v3の場合のみtaskを指定\n",
    "        if model_name == \"jinaai/jina-embeddings-v3\":\n",
    "            train_long_embed = model.encode(\n",
    "                train_long[\"AllText\"].to_list(), task=task, prompt_name=task, normalize_embeddings=True\n",
    "            )\n",
    "            misconception_vec = model.encode(\n",
    "                mapping_meta[\"MisconceptionName_with_SubjectNames\"].to_list(),\n",
    "                task=task,\n",
    "                prompt_name=task,\n",
    "                normalize_embeddings=True,\n",
    "            )\n",
    "\n",
    "        # 埋め込みからTOP100を抽出\n",
    "        top100ids = util.semantic_search(train_long_embed, misconception_vec, top_k=100)\n",
    "\n",
    "        # top100idsから100個のcorpus_id(=misconception_id)を抽出\n",
    "        top100ids_lists = [[qid[\"corpus_id\"] for qid in top100id] for top100id in top100ids]\n",
    "        # gt_misconception_idを抽出\n",
    "        gt_misconception_ids = train_long[\"MisconceptionId\"].to_list()\n",
    "\n",
    "        # 抽出したTOP100の中にgt_misconception_idがあるか確認\n",
    "        is_gt_in_top100 = []\n",
    "        for top100ids_list, gt_misconception_id in zip(top100ids_lists, gt_misconception_ids, strict=True):\n",
    "            if gt_misconception_id in top100ids_list:\n",
    "                is_gt_in_top100.append(True)\n",
    "            else:\n",
    "                is_gt_in_top100.append(False)\n",
    "\n",
    "        # 平均をとってCVスコアとする\n",
    "        avg_score = np.mean(is_gt_in_top100)\n",
    "        cv_scores.append(avg_score)\n",
    "        print(f\"Fold {i+1}: {avg_score}\")\n",
    "\n",
    "    print(f\"CVスコア: {np.mean(cv_scores)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
